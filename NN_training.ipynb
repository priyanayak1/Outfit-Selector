{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD30zWyat4OO",
        "outputId": "3903c7e5-6cd2-48b7-863b-81855c49756d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (0.3.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub) (4.66.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install kagglehub torch torchvision pandas matplotlib scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeCWLrTVwGf6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import resnet18\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "85z0sN9IwI4G",
        "outputId": "2462f2ec-27e8-4e93-d467-1a132cb40883"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-db424f944ac9>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/ML Project Team/dataset'\n",
        "os.chdir(folder_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yx73GC0647Lb",
        "outputId": "30f862d9-6a21-4c29-8aa8-be05b87acf99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_fashion_model.pth\timages\tmyntradataset  styles.csv\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "APPAREL_ARTICLES = [\n",
        "    'Shirts', 'Jeans', 'Track Pants', 'Tshirts', 'Tops', 'Bra', 'Sweatshirts',\n",
        "    'Kurtas', 'Waistcoat', 'Shorts', 'Briefs', 'Sarees', 'Innerwear Vests',\n",
        "    'Rain Jacket', 'Dresses', 'Night suits', 'Skirts', 'Blazers', 'Kurta Sets',\n",
        "    'Shrug', 'Trousers', 'Camisoles', 'Boxers', 'Dupatta', 'Capris', 'Bath Robe',\n",
        "    'Tunics', 'Jackets', 'Trunk', 'Lounge Pants', 'Sweaters', 'Tracksuits',\n",
        "    'Swimwear', 'Nightdress', 'Baby Dolls', 'Leggings', 'Kurtis', 'Jumpsuit',\n",
        "    'Suspenders', 'Robe', 'Salwar and Dupatta', 'Patiala', 'Stockings',\n",
        "    'Tights', 'Churidar', 'Lounge Tshirts', 'Lounge Shorts', 'Shapewear',\n",
        "    'Nehru Jackets', 'Salwar', 'Jeggings', 'Rompers', 'Booties',\n",
        "    'Lehenga Choli', 'Clothing Set', 'Belts', 'Rain Trousers', 'Suits'\n",
        "]\n",
        "\n",
        "FOOTWEAR_ARTICLES = [\n",
        "    'Casual Shoes', 'Flip Flops', 'Sandals', 'Formal Shoes',\n",
        "    'Flats', 'Sports Shoes', 'Heels', 'Sports Sandals'\n",
        "]"
      ],
      "metadata": {
        "id": "TvC5sEckoLQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyLLrUllxE-Z"
      },
      "outputs": [],
      "source": [
        "class FashionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, img_dir, labels_df, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.labels_df = labels_df\n",
        "        self.transform = transform\n",
        "\n",
        "        # Get all unique article types from the dataset\n",
        "        self.classes = sorted(labels_df['articleType'].unique())\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.labels_df.iloc[idx]['id']\n",
        "        img_path = os.path.join('images', f'{img_id}.jpg')\n",
        "\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.class_to_idx[self.labels_df.iloc[idx]['articleType']]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlDge-qPxXJm"
      },
      "outputs": [],
      "source": [
        "class FashionResNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        # use resnet18 from cv\n",
        "        self.model = resnet18(pretrained=True)\n",
        "\n",
        "        # want most of the layers frozen, unfreeze final layer\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        num_ftrs = self.model.fc.in_features\n",
        "        self.model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "        # Print parameter stats\n",
        "        total_params = sum(p.numel() for p in self.model.parameters())\n",
        "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "        print(f'\\nTotal parameters: {total_params:,}')\n",
        "        print(f'Trainable parameters: {trainable_params:,}')\n",
        "        print(f'Frozen parameters: {total_params - trainable_params:,}')\n",
        "\n",
        "        self.loss_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # forward pass for model\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQktEljexZSq"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(loader):\n",
        "        # want to use gpu\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = model.loss_criterion(output, target)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        _, predicted = output.max(1)\n",
        "        total += target.size(0)\n",
        "        correct += predicted.eq(target).sum().item()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Batch: {batch_idx}/{len(loader)} Loss: {loss.item():.4f}')\n",
        "\n",
        "    return total_loss / len(loader), 100. * correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzxPlHFmxg0J"
      },
      "outputs": [],
      "source": [
        "def validate(model, loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = model.loss_criterion(output, target)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = output.max(1)\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "\n",
        "    return total_loss / len(loader), 100. * correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GlCjxWDAb8B"
      },
      "outputs": [],
      "source": [
        "def check_image_paths(df, num_samples=5):\n",
        "    sample_ids = df['id'].head(num_samples)\n",
        "    print(\"\\nChecking image paths:\")\n",
        "\n",
        "    base_paths = [\n",
        "        '/content/drive/MyDrive/ML Project Team/dataset/images',\n",
        "        '/content/drive/.shortcut-targets-by-id/1BrnEUmLFVaEPyjyPpQRW6h7Hy0kqAwAC/ML Project Team/dataset/images',\n",
        "        'images'\n",
        "    ]\n",
        "\n",
        "    for path in base_paths:\n",
        "        print(f\"\\nTrying base path: {path}\")\n",
        "        for img_id in sample_ids:\n",
        "            full_path = os.path.join(path, f'{img_id}.jpg')\n",
        "            exists = os.path.exists(full_path)\n",
        "            print(f\"ID {img_id}: {full_path} - {'Exists' if exists else 'Not found'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzgPj28-xklA"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    # device as gpu\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f'Using device: {device}')\n",
        "\n",
        "    # filter out what we do not want (no accessories)\n",
        "    df = pd.read_csv('styles.csv', on_bad_lines='skip')\n",
        "    df = df[df['masterCategory'].isin(['Apparel', 'Footwear'])]\n",
        "\n",
        "    print(f\"\\nInitial dataset size: {len(df)}\")\n",
        "    print(\"\\nMaster category distribution:\")\n",
        "    print(df['masterCategory'].value_counts())\n",
        "\n",
        "    valid_ids = []\n",
        "    for idx, row in df.iterrows():\n",
        "        if os.path.exists(f'images/{row[\"id\"]}.jpg'):\n",
        "            valid_ids.append(idx)\n",
        "\n",
        "    df = df.loc[valid_ids]\n",
        "\n",
        "    # get rid of any samples that have too little count\n",
        "    article_counts = df['articleType'].value_counts()\n",
        "    valid_articles = article_counts[article_counts >= 30].index\n",
        "    df = df[df['articleType'].isin(valid_articles)]\n",
        "\n",
        "    print(f\"\\nDataset size after filtering: {len(df)}\")\n",
        "    print(\"\\nArticle type distribution:\")\n",
        "    print(df['articleType'].value_counts())\n",
        "    print(\"\\nNumber of unique article types:\", len(df['articleType'].unique()))\n",
        "\n",
        "    # train/test split 0.2\n",
        "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42,\n",
        "                                      stratify=df['articleType'])\n",
        "\n",
        "    # Define transforms\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # make dataset from the splits\n",
        "    train_dataset = FashionDataset('images', train_df, train_transform)\n",
        "    val_dataset = FashionDataset('images', val_df, val_transform)\n",
        "\n",
        "    print(f\"\\nNumber of classes (article types): {len(train_dataset.classes)}\")\n",
        "    print(f\"Article types: {train_dataset.classes}\")\n",
        "    print(f\"Number of training images: {len(train_dataset)}\")\n",
        "    print(f\"Number of validation images: {len(val_dataset)}\")\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=64,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=64,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Initialize model\n",
        "    model = FashionResNet(len(train_dataset.classes)).to(device)\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max',\n",
        "                                                         factor=0.5, patience=3)\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 30\n",
        "    best_val_acc = 0\n",
        "    # Increase patience counter by 1 for each epoch that validation accuracy does\n",
        "    # not increase, after 10 epochs, stop training early.\n",
        "    patience = 10\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # train\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # tqdm for progress bars\n",
        "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
        "        for inputs, labels in pbar:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(inputs)\n",
        "                loss = model.loss_criterion(outputs, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        epoch_acc = 100. * correct / total\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in tqdm(val_loader, desc='Validation'):\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = model.loss_criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_acc = 100. * correct / total\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "        print(f'Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%')\n",
        "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "        scheduler.step(val_acc)\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'train_acc': epoch_acc,\n",
        "                'val_acc': val_acc,\n",
        "                'classes': train_dataset.classes,\n",
        "                'class_to_idx': train_dataset.class_to_idx\n",
        "            }, 'best_fashion_model.pth')\n",
        "            print(f'New best model saved with validation accuracy: {val_acc:.2f}%')\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f'Early stopping triggered after {epoch+1} epochs')\n",
        "            break\n",
        "\n",
        "    print(f'\\nTraining completed. Best validation accuracy: {best_val_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdQWIp9jxwq2",
        "outputId": "f03a3c75-9c80-470a-c7a5-3d5493e04442"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Initial dataset size: 30616\n",
            "\n",
            "Master category distribution:\n",
            "masterCategory\n",
            "Apparel     21397\n",
            "Footwear     9219\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Dataset size after filtering: 12404\n",
            "\n",
            "Article type distribution:\n",
            "articleType\n",
            "Tshirts            2906\n",
            "Shirts             1815\n",
            "Casual Shoes       1191\n",
            "Kurtas              870\n",
            "Sports Shoes        721\n",
            "Tops                576\n",
            "Heels               392\n",
            "Flip Flops          362\n",
            "Sandals             352\n",
            "Briefs              333\n",
            "Trousers            317\n",
            "Sweatshirts         271\n",
            "Sweaters            263\n",
            "Formal Shoes        261\n",
            "Jeans               257\n",
            "Shorts              212\n",
            "Flats               179\n",
            "Jackets             168\n",
            "Innerwear Vests     149\n",
            "Track Pants         145\n",
            "Kurtis              115\n",
            "Bra                 107\n",
            "Trunk                80\n",
            "Dresses              73\n",
            "Leggings             61\n",
            "Kurta Sets           56\n",
            "Capris               55\n",
            "Suspenders           40\n",
            "Tunics               40\n",
            "Skirts               37\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Number of unique article types: 30\n",
            "\n",
            "Number of classes (article types): 30\n",
            "Article types: ['Bra', 'Briefs', 'Capris', 'Casual Shoes', 'Dresses', 'Flats', 'Flip Flops', 'Formal Shoes', 'Heels', 'Innerwear Vests', 'Jackets', 'Jeans', 'Kurta Sets', 'Kurtas', 'Kurtis', 'Leggings', 'Sandals', 'Shirts', 'Shorts', 'Skirts', 'Sports Shoes', 'Suspenders', 'Sweaters', 'Sweatshirts', 'Tops', 'Track Pants', 'Trousers', 'Trunk', 'Tshirts', 'Tunics']\n",
            "Number of training images: 9923\n",
            "Number of validation images: 2481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-43-4572c153c9a5>:82: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total parameters: 11,191,902\n",
            "Trainable parameters: 15,390\n",
            "Frozen parameters: 11,176,512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/30:   0%|          | 0/156 [00:00<?, ?it/s]<ipython-input-43-4572c153c9a5>:105: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Epoch 1/30: 100%|██████████| 156/156 [08:49<00:00,  3.39s/it]\n",
            "Validation: 100%|██████████| 39/39 [02:31<00:00,  3.87s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30:\n",
            "Train Loss: 1.7668, Train Acc: 51.58%\n",
            "Val Loss: 1.1978, Val Acc: 64.01%\n",
            "New best model saved with validation accuracy: 64.01%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/30: 100%|██████████| 156/156 [01:02<00:00,  2.49it/s]\n",
            "Validation: 100%|██████████| 39/39 [00:12<00:00,  3.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/30:\n",
            "Train Loss: 1.0994, Train Acc: 67.16%\n",
            "Val Loss: 0.9664, Val Acc: 69.41%\n",
            "New best model saved with validation accuracy: 69.41%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/30: 100%|██████████| 156/156 [01:02<00:00,  2.50it/s]\n",
            "Validation: 100%|██████████| 39/39 [00:11<00:00,  3.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/30:\n",
            "Train Loss: 0.9483, Train Acc: 70.71%\n",
            "Val Loss: 0.9063, Val Acc: 69.41%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/30: 100%|██████████| 156/156 [01:03<00:00,  2.47it/s]\n",
            "Validation: 100%|██████████| 39/39 [00:11<00:00,  3.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/30:\n",
            "Train Loss: 0.8834, Train Acc: 71.84%\n",
            "Val Loss: 0.8292, Val Acc: 73.40%\n",
            "New best model saved with validation accuracy: 73.40%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/30: 100%|██████████| 156/156 [01:02<00:00,  2.49it/s]\n",
            "Validation: 100%|██████████| 39/39 [00:11<00:00,  3.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/30:\n",
            "Train Loss: 0.8247, Train Acc: 73.29%\n",
            "Val Loss: 0.7902, Val Acc: 73.92%\n",
            "New best model saved with validation accuracy: 73.92%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/30: 100%|██████████| 156/156 [01:02<00:00,  2.51it/s]\n",
            "Validation: 100%|██████████| 39/39 [00:10<00:00,  3.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/30:\n",
            "Train Loss: 0.7828, Train Acc: 74.91%\n",
            "Val Loss: 0.7743, Val Acc: 74.57%\n",
            "New best model saved with validation accuracy: 74.57%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/30: 100%|██████████| 156/156 [01:02<00:00,  2.51it/s]\n",
            "Validation: 100%|██████████| 39/39 [00:11<00:00,  3.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/30:\n",
            "Train Loss: 0.7694, Train Acc: 74.80%\n",
            "Val Loss: 0.7565, Val Acc: 74.77%\n",
            "New best model saved with validation accuracy: 74.77%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/30: 100%|██████████| 156/156 [01:02<00:00,  2.49it/s]\n",
            "Validation: 100%|██████████| 39/39 [00:12<00:00,  3.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/30:\n",
            "Train Loss: 0.7245, Train Acc: 75.91%\n",
            "Val Loss: 0.7294, Val Acc: 75.86%\n",
            "New best model saved with validation accuracy: 75.86%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/30: 100%|██████████| 156/156 [01:02<00:00,  2.48it/s]\n",
            "Validation: 100%|██████████| 39/39 [00:12<00:00,  3.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/30:\n",
            "Train Loss: 0.7013, Train Acc: 77.13%\n",
            "Val Loss: 0.7239, Val Acc: 76.18%\n",
            "New best model saved with validation accuracy: 76.18%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/30: 100%|██████████| 156/156 [01:02<00:00,  2.50it/s]\n",
            "Validation: 100%|██████████| 39/39 [00:12<00:00,  3.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/30:\n",
            "Train Loss: 0.7142, Train Acc: 77.34%\n",
            "Val Loss: 0.6996, Val Acc: 76.78%\n",
            "New best model saved with validation accuracy: 76.78%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/30: 100%|██████████| 156/156 [01:02<00:00,  2.50it/s]\n",
            "Validation: 100%|██████████| 39/39 [00:12<00:00,  3.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/30:\n",
            "Train Loss: 0.6848, Train Acc: 77.60%\n",
            "Val Loss: 0.7120, Val Acc: 75.98%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/30: 100%|██████████| 156/156 [01:02<00:00,  2.49it/s]\n",
            "Validation: 100%|██████████| 39/39 [00:11<00:00,  3.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/30:\n",
            "Train Loss: 0.6665, Train Acc: 77.64%\n",
            "Val Loss: 0.7237, Val Acc: 75.90%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/30: 100%|██████████| 156/156 [01:02<00:00,  2.49it/s]\n",
            "Validation: 100%|██████████| 39/39 [00:10<00:00,  3.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/30:\n",
            "Train Loss: 0.6715, Train Acc: 78.50%\n",
            "Val Loss: 0.6850, Val Acc: 76.70%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/30: 100%|██████████| 156/156 [01:02<00:00,  2.49it/s]\n",
            "Validation: 100%|██████████| 39/39 [00:11<00:00,  3.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/30:\n",
            "Train Loss: 0.6547, Train Acc: 78.12%\n",
            "Val Loss: 0.6841, Val Acc: 77.71%\n",
            "New best model saved with validation accuracy: 77.71%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/30: 100%|██████████| 156/156 [01:02<00:00,  2.50it/s]\n",
            "Validation: 100%|██████████| 39/39 [00:12<00:00,  3.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/30:\n",
            "Train Loss: 0.6450, Train Acc: 78.07%\n",
            "Val Loss: 0.6944, Val Acc: 76.70%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/30: 100%|██████████| 156/156 [01:01<00:00,  2.53it/s]\n",
            "Validation: 100%|██████████| 39/39 [00:12<00:00,  3.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/30:\n",
            "Train Loss: 0.6341, Train Acc: 78.68%\n",
            "Val Loss: 0.6675, Val Acc: 77.27%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/30: 100%|██████████| 156/156 [01:01<00:00,  2.54it/s]\n",
            "Validation: 100%|██████████| 39/39 [00:12<00:00,  3.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/30:\n",
            "Train Loss: 0.6175, Train Acc: 78.89%\n",
            "Val Loss: 0.6704, Val Acc: 77.87%\n",
            "New best model saved with validation accuracy: 77.87%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/30: 100%|██████████| 156/156 [01:01<00:00,  2.53it/s]\n",
            "Validation: 100%|██████████| 39/39 [00:11<00:00,  3.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/30:\n",
            "Train Loss: 0.6377, Train Acc: 79.11%\n",
            "Val Loss: 0.6678, Val Acc: 77.15%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/30: 100%|██████████| 156/156 [01:02<00:00,  2.50it/s]\n",
            "Validation: 100%|██████████| 39/39 [00:11<00:00,  3.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/30:\n",
            "Train Loss: 0.6233, Train Acc: 79.22%\n",
            "Val Loss: 0.6936, Val Acc: 76.18%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/30: 100%|██████████| 156/156 [01:02<00:00,  2.49it/s]\n",
            "Validation: 100%|██████████| 39/39 [00:10<00:00,  3.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/30:\n",
            "Train Loss: 0.6186, Train Acc: 79.09%\n",
            "Val Loss: 0.6560, Val Acc: 77.79%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/30: 100%|██████████| 156/156 [01:02<00:00,  2.49it/s]\n",
            "Validation: 100%|██████████| 39/39 [00:12<00:00,  3.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/30:\n",
            "Train Loss: 0.6032, Train Acc: 79.62%\n",
            "Val Loss: 0.6545, Val Acc: 77.87%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/30: 100%|██████████| 156/156 [01:04<00:00,  2.43it/s]\n",
            "Validation: 100%|██████████| 39/39 [00:12<00:00,  3.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/30:\n",
            "Train Loss: 0.5947, Train Acc: 80.39%\n",
            "Val Loss: 0.6646, Val Acc: 77.51%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/30: 100%|██████████| 156/156 [01:02<00:00,  2.48it/s]\n",
            "Validation: 100%|██████████| 39/39 [00:12<00:00,  3.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/30:\n",
            "Train Loss: 0.5785, Train Acc: 80.62%\n",
            "Val Loss: 0.6549, Val Acc: 77.23%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/30: 100%|██████████| 156/156 [01:02<00:00,  2.51it/s]\n",
            "Validation: 100%|██████████| 39/39 [00:12<00:00,  3.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/30:\n",
            "Train Loss: 0.5836, Train Acc: 79.83%\n",
            "Val Loss: 0.6639, Val Acc: 77.23%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/30: 100%|██████████| 156/156 [01:02<00:00,  2.49it/s]\n",
            "Validation: 100%|██████████| 39/39 [00:12<00:00,  3.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/30:\n",
            "Train Loss: 0.5755, Train Acc: 80.42%\n",
            "Val Loss: 0.6537, Val Acc: 77.71%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/30: 100%|██████████| 156/156 [01:02<00:00,  2.50it/s]\n",
            "Validation: 100%|██████████| 39/39 [00:12<00:00,  3.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/30:\n",
            "Train Loss: 0.5719, Train Acc: 80.85%\n",
            "Val Loss: 0.6575, Val Acc: 78.11%\n",
            "New best model saved with validation accuracy: 78.11%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/30: 100%|██████████| 156/156 [01:02<00:00,  2.50it/s]\n",
            "Validation: 100%|██████████| 39/39 [00:11<00:00,  3.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/30:\n",
            "Train Loss: 0.5728, Train Acc: 80.86%\n",
            "Val Loss: 0.6480, Val Acc: 77.67%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/30: 100%|██████████| 156/156 [01:02<00:00,  2.49it/s]\n",
            "Validation: 100%|██████████| 39/39 [00:11<00:00,  3.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/30:\n",
            "Train Loss: 0.5651, Train Acc: 80.92%\n",
            "Val Loss: 0.6430, Val Acc: 78.15%\n",
            "New best model saved with validation accuracy: 78.15%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/30: 100%|██████████| 156/156 [01:04<00:00,  2.43it/s]\n",
            "Validation: 100%|██████████| 39/39 [00:11<00:00,  3.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/30:\n",
            "Train Loss: 0.5678, Train Acc: 80.85%\n",
            "Val Loss: 0.6361, Val Acc: 78.27%\n",
            "New best model saved with validation accuracy: 78.27%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/30: 100%|██████████| 156/156 [01:03<00:00,  2.46it/s]\n",
            "Validation: 100%|██████████| 39/39 [00:10<00:00,  3.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/30:\n",
            "Train Loss: 0.5677, Train Acc: 81.26%\n",
            "Val Loss: 0.6521, Val Acc: 77.75%\n",
            "\n",
            "Training completed. Best validation accuracy: 78.27%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTNxyEBKMdne",
        "outputId": "458b6f9b-f6c0-4907-b078-7cd53dca07e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-45-50e36f5b2c7b>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  temp_model = torch.load('best_fashion_model.pth')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to: /content/drive/MyDrive/ML Project Team/saved_models/expanded_final_model.pth\n",
            "Saved model contents:\n",
            "- epoch\n",
            "- model_state_dict\n",
            "- optimizer_state_dict\n",
            "- train_acc\n",
            "- val_acc\n",
            "- classes\n",
            "- class_to_idx\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "\n",
        "# save model in drive\n",
        "save_path = '/content/drive/MyDrive/ML Project Team/saved_models/'\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "temp_model = torch.load('best_fashion_model.pth')\n",
        "\n",
        "# new name set for model\n",
        "permanent_save_path = f'{save_path}expanded_final_model.pth'\n",
        "torch.save(temp_model, permanent_save_path)\n",
        "\n",
        "print(f\"Model saved to: {permanent_save_path}\")\n",
        "print(\"Saved model contents:\")\n",
        "for key in temp_model.keys():\n",
        "    print(f\"- {key}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "ZU95BFXbeXdG",
        "outputId": "317a3800-4a29-4f09-d6e5-0258cdd5f5f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'self' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-502572b27adb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}